{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ccd7d96-d9b3-4707-a636-82918634ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b86c18-8aa8-42ed-b2bd-09c58977737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a91185a-1914-486d-b2fd-ee7265009b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model (this may take a while)...\n",
      "Model loaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"falcon_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"falcon_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ falcon_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FalconTokenizer</span>)                            │                       Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ falcon_tokenizer (\u001b[38;5;33mFalconTokenizer\u001b[0m)                            │                       Vocab size: \u001b[38;5;34m50,257\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"falcon_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"falcon_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ falcon_backbone               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,311,625,216</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FalconBackbone</span>)              │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50304</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">103,022,592</span> │ falcon_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ falcon_backbone               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m1,311,625,216\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mFalconBackbone\u001b[0m)              │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50304\u001b[0m)       │     \u001b[38;5;34m103,022,592\u001b[0m │ falcon_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,311,625,216</span> (4.89 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,311,625,216\u001b[0m (4.89 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,311,625,216</span> (4.89 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,311,625,216\u001b[0m (4.89 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled LoRA for efficient fine-tuning with reduced rank.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp  # A Keras-based library for natural language processing tasks.\n",
    "from tensorflow import keras\n",
    "# Mixed Precision Training:\n",
    "# This enables the model to use both 16-bit and 32-bit floating-point types.\n",
    "# Using float16 for most operations reduces memory usage and speeds up computation,\n",
    "# while keeping some operations in float32 maintains stability.\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# ------------------------------\n",
    "# Load the Pre-trained Ollama Model\n",
    "# ------------------------------\n",
    "print(\"Loading model (this may take a while)...\")\n",
    "# This command loads a pre-trained language model named Ollama coder from Hugging Face.\n",
    "# \"Causal\" means the model generates text in a sequential, left-to-right manner.\n",
    "Ollama_coder_model = keras_nlp.models.FalconCausalLM.from_preset(\n",
    "    \"hf://keras/falcon_refinedweb_1b_en\"\n",
    ")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Display the structure of the model, including layers and number of parameters.\n",
    "Ollama_coder_model.summary()\n",
    "\n",
    "# ------------------------------\n",
    "# Enable LoRA Fine-Tuning\n",
    "# ------------------------------\n",
    "# LoRA (Low-Rank Adaptation) is a technique to efficiently fine-tune large models.\n",
    "# Instead of updating every parameter in the model (which can be millions or billions),\n",
    "# LoRA adds smaller matrices with a much lower rank (here, rank=2) to approximate the needed adjustments.\n",
    "# Think of it as fine-tuning by \"tweaking\" only a few parameters instead of re-writing a whole book.\n",
    "Ollama_coder_model.backbone.enable_lora(rank=2)\n",
    "print(\"Enabled LoRA for efficient fine-tuning with reduced rank.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f839c070-ec67-4de8-93a2-5f8caf8158f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 23s/step - loss: 0.1783 - sparse_categorical_accuracy: 0.0242 - weighted_sparse_categorical_accuracy: 0.4137\n",
      "Fine-tuning complete.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Prepare Training Data\n",
    "# ------------------------------\n",
    "# Here, we define a small dataset with pairs of symptoms and corresponding diseases.\n",
    "# Each string follows the format:\n",
    "# \"Symptom: <list of symptoms>.\\nDisease: <disease name>.\"\n",
    "# The \"\\n\" is a newline character that separates the symptoms from the disease.\n",
    "# train_data = [\n",
    "#     \"Symptom: persistent cough, fever, difficulty breathing.\\nDisease: Pneumonia.\",\n",
    "#     \"Symptom: severe headache, neck stiffness, photophobia.\\nDisease: Meningitis.\",\n",
    "#     \"Symptom: sudden weakness on one side, slurred speech.\\nDisease: Stroke.\",\n",
    "#     \"Symptom: increased thirst, frequent urination, unexplained weight loss.\\nDisease: Diabetes.\",\n",
    "#     \"Symptom: joint pain, prolonged morning stiffness, swelling in multiple joints.\\nDisease: Rheumatoid Arthritis.\"\n",
    "# ]\n",
    "\n",
    "# train_data = [\n",
    "#     \"Line Item: Starbucks, $5.67, 2025-02-28, Coffee Shop.\\nLabel: Not Fraud.\",\n",
    "#     \"Line Item: Unknown Merchant, $1200.00, 2025-02-27, Electronics.\\nLabel: Fraud.\",\n",
    "#     \"Line Item: Walmart Supercenter, $45.32, 2025-02-26, Groceries.\\nLabel: Not Fraud.\",\n",
    "#     \"Line Item: Luxury Boutique, $2200.00, 2025-02-28, Designer Clothing.\\nLabel: Fraud.\",\n",
    "#     \"Line Item: Uber, $18.75, 2025-02-27, Ride Share.\\nLabel: Not Fraud.\"\n",
    "# ]\n",
    "\n",
    "train_data = [\n",
    "    \"Question: What is the total quantity (in KG) of all items sold across all companies in the dataset? \\nPandas output: df[df['Unit'] == 'KG']['Quantity'].sum()\",\n",
    "    \"Question: What is the total value (in VND) of all transactions in the dataset? \\nPandas output: (df[df.Currency == 'VND']['Value']).sum()\",\n",
    "    \"Question: How many unique companies are represented in the sales data? \\nPandas output: df['Company_Name'].nunique()\",\n",
    "    \"Question: What is the total value of sales in USD across all transactions? \\nPandas output: df[df['Currency'] == 'USD']['Value'].sum()\",\n",
    "    \"Question: What is the total quantity of items sold in tons (TAN) across all companies? \\nPandas output:df['Quantity'].sum()/1000\",\n",
    "    \"Question: What is the total value of sales (in VND) for Công Ty TNHH Tân Thời in February 2019? \\nPandas output:df[(df['Company_Name'] == 'Công Ty TNHH Tân Thời') & (df['Billing Date'].dt.month == 2) & (df['Billing Date'].dt.year == 2019)]['Value'].sum()\",\n",
    "    \"Question: How many transactions did Công Ty TNHH Sản Xuất Cân Nhơn Hòa record in the dataset? \\nPandas output:df[df['Company_Name'] == 'Công Ty TNHH Sản Xuất Cân Nhơn Hòa']['Billing Date'].count()\",\n",
    "    \"Question: What is the total quantity (in KG) of items sold by Công Ty TNHH Một Thành Viên Quang Min? \\nPandas output:df[(df['Company_Name'] == 'Công Ty TNHH Một Thành Viên Quang Min') & (df['Unit'] == 'KG')]['Quantity'].sum()\",\n",
    "    \"Question: What is the highest-value transaction (in VND) for Công Ty TNHH BOSEUNG VINA? \\nPandas output: [(df['Company_Name'] == 'Công Ty TNHH Một Thành Viên Quang Min') & (df['Currency'] == 'VND')].max()['Value']\",\n",
    "    \"Question: How much did G.I IMPORT EXPORT CO.,LTD spend in USD on transactions dated February 15, 2019?d\\nPandas output: f[(df['Company_Name'] == 'G.I IMPORT EXPORT CO.,LTD') & (df['Billing Date'].dt.month == 2) & (df['Currency']== 'USD')]['Value'].sum()`\",\n",
    "    \"Question: What is the total quantity (in KG) of the item GI 0.65x61/43xC sold by all companies?\\nPandas output: df[df['Item_Details'].str.contains('GI 0.65x61/43xC')]['Quantity'].sum()\",\n",
    "    \"Question: What is the average unit price (VND per KG) of the item PO V 3.0x30x30x3005xC?\\nPandas output: df[df['Item_Details'].str.startswith('PO V 3.0x30x30x3005xC')]['Value'].sum() / df[df['Item_Details'].str.startswith('PO V 3.0x30x30x3005xC')]['Quantity'].sum()\",\n",
    "    \"Question: How many transactions involve the item CR 0.5 x 384 x 734 x 10.000 Tấm?\\nPandas output: df[df['Item_Details'] == 'CR 0.5x384x734x10,000Tấm'].shape[0]\",\n",
    "    \"Question: What is the total value (in VND) of all sales of Thép Mã Kẽm SGCC 0.65x61/43xC?\\nPandas output: df[df['Item_Details'].str.contains('Thép Mã Kẽm SGCC 0.65x61/43xC')]['Value'].sum()\",\n",
    "    \"Question: Which company purchased the most SPCC 0.4 XẢ BĂNG by quantity (in KG)?\\nPandas output: df[df['Item_Details'].str.contains('SPCC 0.4 XẢ BĂNG')]['Company_Name'].value_counts().idxmax()\",\n",
    "]\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Compile the Model\n",
    "# ------------------------------\n",
    "# Before training, the model is compiled by specifying:\n",
    "# - A loss function: Measures how well the model's predictions match the actual labels.\n",
    "# - An optimizer: Determines how the model's weights are updated during training.\n",
    "# - Metrics: Additional measurements to judge performance (here, accuracy).\n",
    "Ollama_coder_model.compile(\n",
    "    # SparseCategoricalCrossentropy is used when you have multiple classes and your labels are integers.\n",
    "    # \"from_logits=True\" indicates that the model's outputs are raw values (logits), not probabilities.\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    # Adam optimizer is chosen for its ability to adjust learning rates during training.\n",
    "    # It combines ideas from momentum and adaptive learning rate techniques.\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    # SparseCategoricalAccuracy computes the percentage of correct predictions.\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Fine-Tune the Model\n",
    "# ------------------------------\n",
    "print(\"Starting fine-tuning...\")\n",
    "# The model is fine-tuned (trained) on the provided training data.\n",
    "# Fine-tuning adjusts the model's weights to specialize in the new task (mapping symptoms to diseases).\n",
    "# A batch size of 1 is used, meaning one training sample is processed at a time.\n",
    "# The training runs for 10 epochs, meaning the model sees the entire dataset 10 times.\n",
    "Ollama_coder_model.fit(train_data, batch_size=1, epochs=1)\n",
    "print(\"Fine-tuning complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c884b8-123a-4981-8b7b-95a3b1813784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Save the Falcon Model\n",
    "# ------------------------------\n",
    "# After training, the model is saved in the recommended .keras format.\n",
    "# This allows you to reuse the model later without retraining.\n",
    "Ollama_coder_model.save(\"Finetuned_Falcon_coder_saleproject.keras\")\n",
    "print(\"Model saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38c0abc-ad84-417e-b6cd-b51eaadf52a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/MacExtend/opt/anaconda3/envs/llms/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:734: UserWarning: `compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
      "  instance.compile_from_config(compile_config)\n",
      "/Volumes/MacExtend/opt/anaconda3/envs/llms/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 294 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded for inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp  # A Keras-based library for natural language processing tasks.\n",
    "from tensorflow import keras\n",
    "\n",
    "# ------------------------------\n",
    "# Reload the Model for Inference\n",
    "# ------------------------------\n",
    "# The saved model is reloaded for performing inference (generating predictions).\n",
    "Ollama_coder_model = keras.models.load_model(\"Finetuned_Falcon_coder_saleproject.keras\")\n",
    "print(\"Model reloaded for inference.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5890178d-7b52-460b-868d-ae7ead47b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Set Up a Sampler for Text Generation\n",
    "# ------------------------------\n",
    "# When generating text, a sampler helps decide the next token (word or subword).\n",
    "# GreedySampler always selects the token with the highest probability at each step.\n",
    "sampler = keras_nlp.samplers.GreedySampler()\n",
    "# The sampler is integrated into the model for use during inference.\n",
    "Ollama_coder_model.compile(sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0766e466-6224-4656-924e-b0c5a386df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas output:\n",
      "Question:  What is the total value of sales (in VND) on February 27, 2019?\n",
      " Pandas output: \n",
      "- The total value of sales (in VND) on February 27, 2019 is \n",
      "- The total value of sales (in VND) on February 27, 2019 is \n",
      "- The total value of sales (in VND) on February 27, 2019 is \n",
      "- The total value of sales (in VND) on February, 2019 is \n",
      "- The total value of sales (in VND) on February, 2019 is \n",
      "- The total value of sales (in VND) on February, 2019 is \n",
      "- The total value of sales (in VND) on February, 2019 is \n",
      "- The total value of sales (in VND) on February is \n",
      "- The total value of sales (in 2019 is \n",
      "- The total sales (in VND) on February is \n",
      "- The total sales (in 2019 is \n",
      "- The total sales (in VND) on February is \n",
      "- The total sales (in 2019 is \n",
      "- The total sales (in 2019 is \n",
      "- The total sales (in 2019 is \n",
      "- The total sales (in 2019 is \n",
      "- The total sales (in 2019 is \n",
      "- The total sales (in 2019) on February is \n",
      "- The total sales (in 2019 is \n",
      "- The total sales (in 2019 is \n",
      "- sales (in 2019)\n",
      "- sales (in 2019 is \n",
      "- sales (in 2019)\n",
      "- sales (in 2019 is \n",
      "- sales (in 2019)\n",
      "- sales (in 2019 is \n",
      "- sales (in 2019)\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019)\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019\n",
      "- sales (in 2019- sales (in 2019\n",
      "- sales (in 2019- sales (in 2019\n",
      "- sales (in 2019- sales (in 2019\n",
      "- sales (in 2019- sales\n",
      "- sales (in 2019-in 2019-in 2019-in 2019\n",
      "- sales\n",
      "- sales (in 2019- sales\n",
      "- sales (in 2019-in 2019- sales\n",
      "- sales\n",
      "- sales (in 2019-in 2019- sales\n",
      "- sales (in 2019- sales\n",
      "- sales (in 2019-in 2019- sales\n",
      "- sales\n",
      "- sales (in 2019- sales (in 2019- sales\n",
      "- sales (in 2019- sales\n",
      "- sales (in 2019- sales\n",
      "- sales (in 2019- sales (in 2019- sales\n",
      "- sales (in 2019- sales (in 2019- sales\n",
      "- sales (in 2019- sales (in 2019- sales\n",
      "- sales (in 2019- sales (in 2019- sales\n",
      "- sales (in 2019- sales- sales- sales- sales- sales- sales\n",
      "- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales- sales-\n"
     ]
    }
   ],
   "source": [
    "# Generate an answer for a given healthcare-related symptom prompt\n",
    "# prompt = \"Symptom: sudden weakness on one side, slurred speech.\\nDisease:\"\n",
    "# prompt = \"Line Item: random merchant, $543.67, 2025-02-31, Retail.\\nLabel:\"\n",
    "prompt = \"Question:  What is the total value of sales (in VND) on February 27, 2019?\\n Pandas output: \"\n",
    "result = Ollama_coder_model.generate(prompt, max_length=1000)\n",
    "print(\"Pandas output:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a111e-b89a-4e2a-971c-75610beba146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
